{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll be using the 500 document Brown corpus included in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is divided up into two independent parts: the first uses PMI for distinguishing good collocations, and the second involves building a vector space model for document retrieval.\n",
    "\n",
    "For the PMI portion, we'll use a function which extracts the information we need for a particular two word collocation, namely counts of each word individually, counts of the collocation, and the total number of word tokens in the corpus, and then calculates the PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_PMI_for_collocation_brown(word1,word2):\n",
    "    word1_count = 0\n",
    "    word2_count = 0\n",
    "    both_count = 0\n",
    "    total_count = 0.0 # so that division results in a float\n",
    "    for sent in brown.sents():\n",
    "        sent = [word.lower() for word in sent]\n",
    "        for i in range(len(sent)):\n",
    "            total_count += 1\n",
    "            if sent[i] == word1:\n",
    "                word1_count += 1\n",
    "                if i < len(sent) - 1 and sent[i + 1] == word2:\n",
    "                    both_count += 1\n",
    "            elif sent[i] == word2:\n",
    "                word2_count += 1\n",
    "    return math.log((both_count/total_count)/((word1_count/total_count)*(word2_count/total_count)), 2)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in a typical use case, we probably wouldn't do it this way, since we'd likely want to calculate PMI across many different words, and collecting the statisitcs for this can be done in a single pass across the corpus for all words, and then the PMI calculated in a separate function. Anyway, let's compare the PMI for two phrases, \"hard work\" and \"some work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.237244531670497\n",
      "1.9135320271049516\n"
     ]
    }
   ],
   "source": [
    "print(get_PMI_for_collocation_brown(\"hard\",\"work\"))\n",
    "print(get_PMI_for_collocation_brown(\"some\",\"work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on PMI, \"hard work\" appears to be a much better collocation than \"some work\", which matches our intuition. Go ahead and try out this out some other collocations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the notebook, let's create a sparse document-term matrix, using sci-kit learn. We will do a document-term matrix rather than term-document because we will be performing SVD dimensionality reduction to produce dense document representations for document retrevial. Note that this is actually identical to creating a BOW feature representation for each document; the difference comes in how we used the representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49)\t1.0\n",
      "  (0, 58)\t1.0\n",
      "  (0, 169)\t1.0\n",
      "  (0, 181)\t1.0\n",
      "  (0, 205)\t1.0\n",
      "  (0, 238)\t1.0\n",
      "  (0, 322)\t33.0\n",
      "  (0, 373)\t3.0\n",
      "  (0, 374)\t3.0\n",
      "  (0, 393)\t87.0\n",
      "  (0, 395)\t4.0\n",
      "  (0, 405)\t88.0\n",
      "  (0, 454)\t4.0\n",
      "  (0, 465)\t1.0\n",
      "  (0, 695)\t1.0\n",
      "  (0, 720)\t1.0\n",
      "  (0, 939)\t1.0\n",
      "  (0, 1087)\t1.0\n",
      "  (0, 1103)\t1.0\n",
      "  (0, 1123)\t1.0\n",
      "  (0, 1159)\t1.0\n",
      "  (0, 1170)\t1.0\n",
      "  (0, 1173)\t1.0\n",
      "  (0, 1200)\t3.0\n",
      "  (0, 1451)\t1.0\n",
      "  :\t:\n",
      "  (499, 49161)\t1.0\n",
      "  (499, 49164)\t1.0\n",
      "  (499, 49242)\t1.0\n",
      "  (499, 49253)\t1.0\n",
      "  (499, 49275)\t1.0\n",
      "  (499, 49301)\t1.0\n",
      "  (499, 49313)\t1.0\n",
      "  (499, 49369)\t1.0\n",
      "  (499, 49385)\t1.0\n",
      "  (499, 49386)\t4.0\n",
      "  (499, 49390)\t2.0\n",
      "  (499, 49410)\t2.0\n",
      "  (499, 49446)\t1.0\n",
      "  (499, 49576)\t1.0\n",
      "  (499, 49590)\t1.0\n",
      "  (499, 49613)\t3.0\n",
      "  (499, 49691)\t42.0\n",
      "  (499, 49694)\t3.0\n",
      "  (499, 49697)\t3.0\n",
      "  (499, 49698)\t1.0\n",
      "  (499, 49707)\t17.0\n",
      "  (499, 49708)\t1.0\n",
      "  (499, 49710)\t4.0\n",
      "  (499, 49711)\t1.0\n",
      "  (499, 49797)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "\n",
    "texts = []\n",
    "for fileid in brown.fileids():\n",
    "    texts.append(get_BOW(brown.words(fileid)))\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "brown_matrix = vectorizer.fit_transform(texts)\n",
    "print(brown_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrix is sparse: for instance, columns 0-48 in row 0 are empty, and are just left out, only the rows and columns with values other than zeros are displayed\n",
    "\n",
    "Rather than removing stopwords as we did for text classification, let's add some idf weighting to this matrix. Scikit-learn has a built-in tf-idf transformer for just this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49646)\t1.72981116493\n",
      "  (0, 49613)\t1.36816932336\n",
      "  (0, 49596)\t3.70663186543\n",
      "  (0, 49386)\t9.98833379406\n",
      "  (0, 49378)\t8.73162901565\n",
      "  (0, 49313)\t2.62964061975\n",
      "  (0, 49301)\t7.37407593121\n",
      "  (0, 49292)\t2.18417017703\n",
      "  (0, 49224)\t3.38596670193\n",
      "  (0, 49147)\t6.0\n",
      "  (0, 49041)\t3.40794560865\n",
      "  (0, 49003)\t22.2100968809\n",
      "  (0, 49001)\t5.74160535314\n",
      "  (0, 48990)\t16.8467729363\n",
      "  (0, 48951)\t4.72970144863\n",
      "  (0, 48950)\t4.93935194012\n",
      "  (0, 48932)\t3.9565115604\n",
      "  (0, 48867)\t7.04612032287\n",
      "  (0, 48777)\t1.41855034766\n",
      "  (0, 48771)\t13.6942100975\n",
      "  (0, 48769)\t6.23642898412\n",
      "  (0, 48753)\t1.29571424415\n",
      "  (0, 48749)\t3.19841940751\n",
      "  (0, 48720)\t1.16487464319\n",
      "  (0, 48670)\t2.19743194588\n",
      "  :\t:\n",
      "  (499, 2710)\t3.1202635362\n",
      "  (499, 2688)\t2.04412410338\n",
      "  (499, 2670)\t3.9565115604\n",
      "  (499, 2611)\t4.27016911926\n",
      "  (499, 2468)\t6.52146091786\n",
      "  (499, 2439)\t4.1700856607\n",
      "  (499, 2415)\t4.12263300785\n",
      "  (499, 2413)\t2.32033750431\n",
      "  (499, 2388)\t2.09661428601\n",
      "  (499, 2358)\t6.11599580975\n",
      "  (499, 2290)\t61.0\n",
      "  (499, 2289)\t7.55330245138\n",
      "  (499, 2286)\t11.1562013446\n",
      "  (499, 2285)\t20.7148120152\n",
      "  (499, 2283)\t1.22564668153\n",
      "  (499, 1345)\t6.52146091786\n",
      "  (499, 1141)\t4.50655789732\n",
      "  (499, 405)\t83.0\n",
      "  (499, 395)\t12.7103339312\n",
      "  (499, 393)\t188.0\n",
      "  (499, 374)\t4.08721685594\n",
      "  (499, 373)\t4.09584995543\n",
      "  (499, 354)\t7.21460809842\n",
      "  (499, 322)\t7.53816731035\n",
      "  (499, 320)\t3.47693848014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "brown_matrix_tfidf = transformer.fit_transform(brown_matrix)\n",
    "\n",
    "print(brown_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply SVD. Scikit-learn does not expose the internal details of the decomposition, we just use the [TruncatedSVD class](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) directly get a matrix with k dimensions. Since the Brown corpus is a fairly small corpus, we'll do k=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 100)\n",
      "[[ 242.5582075    22.94672559   -9.13296898 ...,   -6.23957857\n",
      "     5.39073838   -6.7454443 ]\n",
      " [ 248.38544615   25.02117041  -21.15649044 ...,   -8.00926973\n",
      "     6.68446001   -4.66650109]\n",
      " [ 236.70180717   24.01329017  -11.14248995 ...,   -6.4456885    -8.13839828\n",
      "     3.96625212]\n",
      " ..., \n",
      " [ 258.64888365 -113.70379768   26.56777272 ...,    7.37187006\n",
      "    -0.96696039  -10.34101781]\n",
      " [ 291.34612775   12.99993635  -26.75489983 ...,   -4.27981877    5.24324\n",
      "     4.38317943]\n",
      " [ 273.31546131  -31.90748229  -17.78595109 ...,   -5.03546029\n",
      "    -6.22586026   -2.21868552]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "brown_matrix_lowrank = svd.fit_transform(brown_matrix_tfidf)\n",
    "\n",
    "print(brown_matrix_lowrank.shape)\n",
    "print(brown_matrix_lowrank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned matrix corresponds to the transformed documents, $U \\Sigma$, after SVD factorisation, $X \\approx U \\Sigma V^T$, applied to `brown_matrix_tfidf`, as $X$. Note that the resulting matrix is not sparse.\n",
    "\n",
    "The last thing we'll do is build a very simple document retrevial system based on the vector space model we've built: it will take some query input, apply all the transformations we have defined above, then find the Brown document with the highest cosine similarity to the query document. Here we are using scipy's cosine distance function; we actually find the smallest distance instead of the largest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_query(query_text):\n",
    "    return svd.transform(transformer.transform(vectorizer.transform([get_BOW(query_text.split())])))[0]\n",
    "\n",
    "def get_best_doc_num(query, m = brown_matrix_lowrank):\n",
    "    dists = np.dot(m, query) / np.sqrt(np.einsum('ij,ij->i', m, m))\n",
    "    # the above finds q . m[i] for all rows, then normalises (element-wise) by each row's 2-norm, m[i].m[i]\n",
    "    best_doc = np.argmax(dists)\n",
    "    return best_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out with a couple of sets of key words, with the idea of getting a religious text in the first example, and a mathematics text in the second (the Brown corpus has both). We'll also look at the specific vectors and distances involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query text\n",
      "heaven hell devil lord\n",
      "query vector\n",
      "[  2.54452687e-02  -7.56679974e-02   1.41123579e-02   2.74106732e-05\n",
      "   4.26817295e-02  -1.00728318e-01  -4.57309759e-02   3.57125996e-02\n",
      "   5.52501704e-02   4.08454940e-02   2.68765592e-02  -4.91123930e-02\n",
      "   7.18247112e-02   4.36630663e-03  -4.13951850e-02   4.52565463e-02\n",
      "  -1.23907966e-01   1.44283934e-02   3.10530881e-03   2.71884851e-02\n",
      "  -7.96231396e-02   8.05651038e-03   9.33174406e-02  -3.21580624e-02\n",
      "   7.70688069e-02  -1.50288779e-02   2.54168433e-03  -2.92396752e-02\n",
      "   1.29587920e-01  -5.56075600e-02  -6.85467199e-02   8.06581653e-03\n",
      "   5.60641633e-02   3.48097625e-02  -8.53515946e-02   1.19141345e-02\n",
      "   1.10413528e-02  -1.30118767e-03   2.58580887e-02  -4.56148746e-02\n",
      "   8.02522735e-02  -1.40909593e-01  -2.94006023e-02  -6.41316680e-02\n",
      "  -4.85668189e-02  -3.77222269e-02   1.79331027e-02  -5.39532233e-02\n",
      "  -7.60884593e-02  -9.01020597e-02  -6.36670784e-02   1.41391452e-01\n",
      "  -4.41859967e-02  -8.52973446e-02   5.05782326e-02  -7.82294835e-02\n",
      "  -5.13033794e-02  -4.37743625e-02   6.18478194e-02   4.00326493e-02\n",
      "   8.56729845e-02   3.52926164e-02  -2.54772159e-03   3.31759887e-02\n",
      "   4.62270775e-02  -2.56409980e-02   1.49012127e-02  -2.05920442e-02\n",
      "  -2.08657018e-01  -3.98689928e-02  -5.84348075e-04   4.13989864e-02\n",
      "  -2.33238763e-02  -6.03181671e-02  -3.65460981e-02   5.20715057e-03\n",
      "  -6.02146012e-02   6.32941101e-02   4.49587668e-02  -2.63106536e-02\n",
      "   1.58996760e-01  -7.63352568e-02   6.44993047e-02   8.06836523e-02\n",
      "  -8.06745387e-03  -1.53749181e-01  -4.94322725e-02   5.72937816e-02\n",
      "   6.29637280e-02   8.83995437e-02  -7.64096513e-02   1.19892876e-01\n",
      "  -3.20682052e-02  -1.63118096e-03   3.26511654e-02   9.89898828e-03\n",
      "  -3.77344727e-02   2.03614425e-02  -7.88891079e-03   5.45913617e-02]\n",
      "best document vector\n",
      "[ 286.51647623   -9.00863385   -0.72305901   23.53699847   50.64611776\n",
      "  -10.97267325  -46.61934799  -16.43006191    6.35126488   33.90350174\n",
      "    0.6378871   -23.02763455   21.54246507  -14.90587381  -10.76942832\n",
      "   13.04743973  -25.51075413    2.80149047  -14.11688716    7.48131344\n",
      "  -40.91234081    7.31952988   12.22538777  -23.0209943    54.07154049\n",
      "    3.16254734  -35.21116348  -31.60752129   42.20371616  -34.48993493\n",
      "  -13.12441209   28.07661172   12.67562293   14.09853146  -27.30104648\n",
      "   -5.69901813   22.89165806   15.30684954   15.3428684   -38.59529532\n",
      "   49.11806036  -58.34498679   11.86290052  -64.96144892    8.4826691\n",
      "    3.18396828    2.65293476   13.34858426  -14.73889015  -35.09858778\n",
      "  -10.03521739   59.20275812  -37.37062397  -32.51395219   43.07978187\n",
      "  -56.3373552    -7.49168156  -18.99416039   31.99008973   11.12736048\n",
      "   54.19806706   28.02650387    4.10761541   -2.45653464   16.3233354\n",
      "   -9.01616164   18.29095935  -19.39267953  -50.94310862  -27.9651214\n",
      "   -3.09955337    6.14286309    9.60895766    8.28689834  -14.13159817\n",
      "  -22.63855773   -4.19265363  -19.68817551  -28.72725609  -23.06205449\n",
      "   11.23638745    9.24682012    7.4005282    25.20916264  -17.98819952\n",
      "  -30.96287845   13.15850248   12.63521536   32.00866677   11.05455439\n",
      "    9.21341111   27.0756841   -15.00839836   -4.91636297  -20.5333607\n",
      "   11.67748421   11.60502479   19.80075266    2.27604465   -6.23588401]\n",
      "cosine similarity from query to document\n",
      "0.487156800931\n",
      "best document sample\n",
      "['Pope', 'Leo', '13', ',', 'on', 'the', '13th', 'day', 'of', 'December', '1898', ',', 'granted', 'the', 'following', 'indulgences', ':', '``', 'An', 'indulgence', 'of', 'three', 'hundred', 'days', 'is', 'granted', 'to', 'all', 'the', 'Faithful', 'who', 'read', 'the', 'Holy', 'Gospels', 'at', 'least', 'a', 'quarter', 'of', 'an', 'hour', '.', 'A', 'Plenary', 'Indulgence', 'under', 'the', 'usual', 'conditions']\n",
      "query text\n",
      "matrix algebra eigenvalue\n",
      "query vector\n",
      "[ 0.00120706  0.01168865  0.06325555  0.02990177 -0.00532708 -0.01350265\n",
      " -0.01577596  0.00657862 -0.0194392  -0.01589456  0.01911442  0.00179095\n",
      " -0.02056563 -0.02880845  0.00790888 -0.07702999 -0.06470242  0.01232424\n",
      "  0.01814611  0.01123012  0.03234629  0.00123689 -0.03467179  0.01392154\n",
      "  0.00460874 -0.00269748  0.00530143 -0.02759379  0.00195914 -0.03211335\n",
      "  0.02389763  0.01769835 -0.02163234  0.00736072 -0.00163157  0.00986843\n",
      " -0.00945072  0.02128767 -0.02026702  0.01254245 -0.01783306  0.00362994\n",
      "  0.01068431 -0.01480324 -0.0104556  -0.00544791  0.02212157 -0.00712596\n",
      "  0.01454523  0.01007281 -0.03879249  0.00445539  0.01143859 -0.00702285\n",
      "  0.01757769  0.0215366  -0.00305587 -0.01195419 -0.06446018  0.00159192\n",
      "  0.02259256  0.01381269 -0.01540742 -0.01999785  0.01979116 -0.00652379\n",
      "  0.00823837  0.01311381  0.01719864  0.00060091 -0.00351153  0.01279949\n",
      " -0.0067846   0.00648336  0.00651448  0.00380705 -0.01134703  0.01723675\n",
      "  0.00719175  0.00825702  0.00589982  0.01093505  0.00389576  0.00739042\n",
      " -0.00663148 -0.00209346 -0.01897145  0.01084818 -0.00316024  0.01241839\n",
      "  0.00290035  0.00900443  0.01240955  0.00140056 -0.01420418 -0.00696531\n",
      " -0.00730195  0.00238368 -0.00560015 -0.00540792]\n",
      "best document vector\n",
      "[  2.65887399e+02   1.90805124e+02   5.66319146e+02   1.90258171e+02\n",
      "  -2.24147282e+01  -6.63868038e+01  -6.45492627e+01   2.53371849e+01\n",
      "  -6.99765282e+01  -4.96430877e+01   5.32453269e+01   8.84911586e+00\n",
      "  -4.16765525e+01  -7.99329586e+01   2.36446459e+01  -1.72591312e+02\n",
      "  -1.34988228e+02   1.84179876e+01   3.63055740e+01   1.49480485e+01\n",
      "   6.25115647e+01   1.13287471e+01  -5.50012384e+01   1.61776030e+01\n",
      "   1.04973700e+01  -6.87342560e+00   1.16107403e+01  -5.49316192e+01\n",
      "  -1.39813057e+00  -4.71609225e+01   3.98003096e+01   2.99182553e+01\n",
      "  -3.77935746e+01   1.94552611e+01  -4.82071047e-01   1.37866120e+01\n",
      "  -3.34344685e-01   3.24206447e+01  -3.10637953e+01   1.56174232e+01\n",
      "  -3.16034850e+01   2.59156308e+00   1.36789477e+01  -2.51161099e+01\n",
      "  -1.16867692e+01  -5.76074601e+00   2.23460431e+01  -1.30018197e+01\n",
      "   1.84802562e+01   1.93556280e+01  -5.42575532e+01   1.26566634e+01\n",
      "   1.89587132e+01  -8.49096792e+00   2.32189644e+01   2.78096727e+01\n",
      "  -1.25059555e+01  -1.36963351e+01  -7.38576013e+01  -1.99747655e+00\n",
      "   2.61974574e+01   1.13179225e+01  -1.91220964e+01  -1.00538635e+01\n",
      "   1.05306478e+01  -1.29311939e+01   1.08442408e+01   2.44223723e+01\n",
      "   1.01843467e+01   4.09533280e+00  -2.03394904e+01   1.59647440e+01\n",
      "   5.59900779e+00   1.18443834e+01  -9.80668999e+00  -3.39046335e+00\n",
      "  -8.18141486e-01   7.73193579e+00   4.08238928e+00   8.70323736e+00\n",
      "   4.33675851e+00   5.54256358e+00   1.12197879e+00   1.39301085e+01\n",
      "  -8.94948209e+00   1.00054928e+00  -9.38782579e+00   1.13638417e+01\n",
      "   1.31898080e+00   6.17799703e+00   2.87007977e+00   8.88937832e+00\n",
      "   1.03894972e+01   7.75272987e-01  -2.24394274e+00  -3.02579357e+00\n",
      "   6.64246768e-01   1.24669958e+00   5.78113796e+00   8.04382310e+00]\n",
      "cosine similarity from query to document\n",
      "0.677112685419\n",
      "best document sample\n",
      "['6.4', '.', 'The', 'primary', 'decomposition', 'theorem', 'We', 'are', 'trying', 'to', 'study', 'a', 'linear', 'operator', 'T', 'on', 'the', 'finite-dimensional', 'space', 'V', ',', 'by', 'decomposing', 'T', 'into', 'a', 'direct', 'sum', 'of', 'operators', 'which', 'are', 'in', 'some', 'sense', 'elementary', '.', 'We', 'can', 'do', 'this', 'through', 'the', 'characteristic', 'values', 'and', 'vectors', 'of', 'T', 'in']\n"
     ]
    }
   ],
   "source": [
    "def try_query(query_text):\n",
    "    query = transform_query(query_text)\n",
    "    doc_num = get_best_doc_num(query)\n",
    "    doc_vec = brown_matrix_lowrank[doc_num]\n",
    "    doc_text = brown.words(brown.fileids()[doc_num])\n",
    "    print(\"query text\")\n",
    "    print(query_text)\n",
    "    print(\"query vector\")\n",
    "    print(query)\n",
    "    print(\"best document vector\")\n",
    "    print(doc_vec)\n",
    "    print(\"cosine similarity from query to document\")\n",
    "    print(np.dot(doc_vec, query) / np.sqrt(np.dot(doc_vec,doc_vec) * np.dot(query,query)))\n",
    "    print(\"best document sample\")\n",
    "    print(doc_text[:50])\n",
    "\n",
    "try_query(\"heaven hell devil lord\")\n",
    "try_query(\"matrix algebra eigenvalue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
