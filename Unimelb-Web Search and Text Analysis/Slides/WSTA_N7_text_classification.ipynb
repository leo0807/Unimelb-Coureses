{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the corpus we will be using, which is included in NLTK. You will need NLTK and Scikit-learn (as well as their dependencies, in particular scipy and numpy) to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\winnc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\") # if necessary\n",
    "from nltk.corpus import reuters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK sample of the Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and is divided into a training and test sets, a split which we will preserve here. Let's look at the counts of texts the various categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acq 2369\n",
      "alum 58\n",
      "barley 51\n",
      "bop 105\n",
      "carcass 68\n",
      "castor-oil 2\n",
      "cocoa 73\n",
      "coconut 6\n",
      "coconut-oil 7\n",
      "coffee 139\n",
      "copper 65\n",
      "copra-cake 3\n",
      "corn 237\n",
      "cotton 59\n",
      "cotton-oil 3\n",
      "cpi 97\n",
      "cpu 4\n",
      "crude 578\n",
      "dfl 3\n",
      "dlr 175\n",
      "dmk 14\n",
      "earn 3964\n",
      "fuel 23\n",
      "gas 54\n",
      "gnp 136\n",
      "gold 124\n",
      "grain 582\n",
      "groundnut 9\n",
      "groundnut-oil 2\n",
      "heat 19\n",
      "hog 22\n",
      "housing 20\n",
      "income 16\n",
      "instal-debt 6\n",
      "interest 478\n",
      "ipi 53\n",
      "iron-steel 54\n",
      "jet 5\n",
      "jobs 67\n",
      "l-cattle 8\n",
      "lead 29\n",
      "lei 15\n",
      "lin-oil 2\n",
      "livestock 99\n",
      "lumber 16\n",
      "meal-feed 49\n",
      "money-fx 717\n",
      "money-supply 174\n",
      "naphtha 6\n",
      "nat-gas 105\n",
      "nickel 9\n",
      "nkr 3\n",
      "nzdlr 4\n",
      "oat 14\n",
      "oilseed 171\n",
      "orange 27\n",
      "palladium 3\n",
      "palm-oil 40\n",
      "palmkernel 3\n",
      "pet-chem 32\n",
      "platinum 12\n",
      "potato 6\n",
      "propane 6\n",
      "rand 3\n",
      "rape-oil 8\n",
      "rapeseed 27\n",
      "reserves 73\n",
      "retail 25\n",
      "rice 59\n",
      "rubber 49\n",
      "rye 2\n",
      "ship 286\n",
      "silver 29\n",
      "sorghum 34\n",
      "soy-meal 26\n",
      "soy-oil 25\n",
      "soybean 111\n",
      "strategic-metal 27\n",
      "sugar 162\n",
      "sun-meal 2\n",
      "sun-oil 7\n",
      "sunseed 16\n",
      "tea 13\n",
      "tin 30\n",
      "trade 485\n",
      "veg-oil 124\n",
      "wheat 283\n",
      "wpi 29\n",
      "yen 59\n",
      "zinc 34\n"
     ]
    }
   ],
   "source": [
    "for category in reuters.categories():\n",
    "    print (category, len(reuters.fileids(category)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the documents in the corpus are tagged with multiple labels; in this situation, a straightforward approach is to build a classifier for each label. Let's build a classifier to distinguish the most common topic in the corpus, \"acq\" (acqusitions). First, here's some code to build the dataset in preparation for classification using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_reuters_data(topic,feature_extractor):\n",
    "    training_set = []\n",
    "    training_classifications = []\n",
    "    test_set = []\n",
    "    test_classifications = []\n",
    "    for file_id in reuters.fileids():\n",
    "        feature_dict = feature_extractor(reuters.words(file_id))   \n",
    "        if file_id.startswith(\"train\"):\n",
    "            training_set.append(feature_dict)\n",
    "            if topic in reuters.categories(file_id):\n",
    "                training_classifications.append(topic)\n",
    "            else:\n",
    "                training_classifications.append(\"not \" + topic)\n",
    "        else:\n",
    "            test_set.append(feature_dict)\n",
    "            if topic in reuters.categories(file_id):\n",
    "                test_classifications.append(topic)\n",
    "            else:\n",
    "                test_classifications.append(\"not \" + topic)        \n",
    "    vectorizer = DictVectorizer()\n",
    "    training_data = vectorizer.fit_transform(training_set)\n",
    "    test_data = vectorizer.transform(test_set)\n",
    "    return training_data,training_classifications,test_data,test_classifications\n",
    "\n",
    "trn_data,trn_classes,test_data,test_classes = prepare_reuters_data(\"acq\",get_BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code builds a sparse bag of words feature representation (a Python dictionary) for each text in the corpus (which is pre-tokenized), and places it the appropriate list depending on whether it is testing or training; a corresponding list of correct classifications is created at the same time. The scikit-learn DictVectorizer class converts Python dictionaries into the scipy sparse matrices which Scikit-learn uses; for the training set, use the fit_transform method (which fixes the total number of features in the model), and for the test set, use transform method (which ignores any features in the test set that weren't in the training set). Next, let's prepare some classifiers to test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),\n",
    "        MultinomialNB(),LinearSVC(),LogisticRegression()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we are using default settings for all these classifiers. Let's start by doing 10-fold crossvalidation on the training set, and looking at the accuracy, recall, precision, and f1-score for each (be patient, this may take a while to complete)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "accuracy\n",
      "0.9258591839361565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.85      0.79      0.82      1650\n",
      "     not acq       0.95      0.96      0.95      6119\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7769\n",
      "   macro avg       0.90      0.88      0.89      7769\n",
      "weighted avg       0.92      0.93      0.92      7769\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "accuracy\n",
      "0.9358990861114687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.85      0.85      0.85      1650\n",
      "     not acq       0.96      0.96      0.96      6119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      7769\n",
      "   macro avg       0.90      0.90      0.90      7769\n",
      "weighted avg       0.94      0.94      0.94      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "accuracy\n",
      "0.9474835886214442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.89      0.86      0.87      1650\n",
      "     not acq       0.96      0.97      0.97      6119\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      7769\n",
      "   macro avg       0.92      0.92      0.92      7769\n",
      "weighted avg       0.95      0.95      0.95      7769\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "accuracy\n",
      "0.9549491569056506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.84      0.98      0.90      1650\n",
      "     not acq       0.99      0.95      0.97      6119\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      7769\n",
      "   macro avg       0.92      0.96      0.94      7769\n",
      "weighted avg       0.96      0.95      0.96      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "accuracy\n",
      "0.9769597116746042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.94      0.95      1650\n",
      "     not acq       0.98      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.96      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9769597116746042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.94      0.95      1650\n",
      "     not acq       0.98      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.96      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    for clf in clfs:\n",
    "        predictions = model_selection.cross_val_predict(clf, data,classifications, cv=10)\n",
    "        print (clf)\n",
    "        print (\"accuracy\")\n",
    "        print (accuracy_score(classifications,predictions))\n",
    "        print (classification_report(classifications,predictions))\n",
    "        \n",
    "do_multiple_10foldcrossvalidation(clfs,trn_data,trn_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the classifiers are not obviously biased towards a particular task, so accuracy and f-score are nearly the same. The numbers are generally quite high, indicating that it is a fairly easy classification task. In terms of the best classifier, the clear standouts here are the SVM and Logistic Regression classifiers, while <i>k</i>NN is clearly the worst. One reason <i>k</i>NN might be doing poorly is that it is particularly susceptible to a noisy feature space with dimensions that are irrelevant to the task. Let's try to improve performance by removing stopwords and doing lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "accuracy\n",
      "0.9387308533916849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.90      0.80      0.85      1650\n",
      "     not acq       0.95      0.98      0.96      6119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      7769\n",
      "   macro avg       0.92      0.89      0.90      7769\n",
      "weighted avg       0.94      0.94      0.94      7769\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "accuracy\n",
      "0.9414339039773458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.86      0.86      0.86      1650\n",
      "     not acq       0.96      0.96      0.96      6119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      7769\n",
      "   macro avg       0.91      0.91      0.91      7769\n",
      "weighted avg       0.94      0.94      0.94      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "accuracy\n",
      "0.9549491569056506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.89      0.90      0.89      1650\n",
      "     not acq       0.97      0.97      0.97      6119\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      7769\n",
      "   macro avg       0.93      0.94      0.93      7769\n",
      "weighted avg       0.96      0.95      0.96      7769\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "accuracy\n",
      "0.9561076071566482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.84      0.98      0.90      1650\n",
      "     not acq       0.99      0.95      0.97      6119\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      7769\n",
      "   macro avg       0.92      0.96      0.94      7769\n",
      "weighted avg       0.96      0.96      0.96      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "accuracy\n",
      "0.9787617453983781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.95      0.95      1650\n",
      "     not acq       0.99      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.97      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9797914789548204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.96      0.95      0.95      1650\n",
      "     not acq       0.99      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.97      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def get_BOW_lowered_no_stopwords(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "trn_data,trn_classes,test_data,test_classes = prepare_reuters_data(\"acq\",get_BOW_lowered_no_stopwords)\n",
    "\n",
    "do_multiple_10foldcrossvalidation(clfs,trn_data,trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did improve the performance of <i>k</i>NN by about 1% accuracy, but it is still the worst classifier. Gains for other classifiers were more modest, since the scores were already high, and those classifiers are more robust to feature noise.\n",
    "\n",
    "The random forest classifier is doing worse than its reputation would suggest. The default number of decision trees (n_estimators) used in the model is only 10, which is fairly low: lets see if we can find a better number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "accuracy\n",
      "0.9507015059853263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.89      0.88      0.88      1650\n",
      "     not acq       0.97      0.97      0.97      6119\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      7769\n",
      "   macro avg       0.93      0.92      0.93      7769\n",
      "weighted avg       0.95      0.95      0.95      7769\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "accuracy\n",
      "0.9660187926374051\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.94      0.90      0.92      1650\n",
      "     not acq       0.97      0.98      0.98      6119\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      7769\n",
      "   macro avg       0.96      0.94      0.95      7769\n",
      "weighted avg       0.97      0.97      0.97      7769\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "accuracy\n",
      "0.9675633929720685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.94      0.91      0.92      1650\n",
      "     not acq       0.97      0.98      0.98      6119\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      7769\n",
      "   macro avg       0.96      0.95      0.95      7769\n",
      "weighted avg       0.97      0.97      0.97      7769\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "accuracy\n",
      "0.965503925859184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.94      0.90      0.92      1650\n",
      "     not acq       0.97      0.98      0.98      6119\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      7769\n",
      "   macro avg       0.96      0.94      0.95      7769\n",
      "weighted avg       0.97      0.97      0.97      7769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_to_test = [10,50,100,150]\n",
    "rfs = [RandomForestClassifier(n_estimators=n) for n in n_to_test]\n",
    "do_multiple_10foldcrossvalidation(rfs,trn_data,trn_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, more subclassifiers improved things, though the Random Forest classifier is still slightly inferior to the SVM and Logistic Regression classifiers in this BOW (i.e. large feature set) situation. \n",
    "\n",
    "Both SVM and Logistic Regression classifiers have a C parameter which controls the degree of regularization (lower C means more emphasis on regularization when optimising the model). Let's see if we can improve the performance of the Logistic Regression classifier by changing the C parameter from the default (1.0). For this parameter, a logrithmic scale is appropriate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9530184064873214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.92      0.85      0.88      1650\n",
      "     not acq       0.96      0.98      0.97      6119\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      7769\n",
      "   macro avg       0.94      0.92      0.93      7769\n",
      "weighted avg       0.95      0.95      0.95      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.971424893808727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.94      0.92      0.93      1650\n",
      "     not acq       0.98      0.99      0.98      6119\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      7769\n",
      "   macro avg       0.96      0.95      0.96      7769\n",
      "weighted avg       0.97      0.97      0.97      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9773458617582701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.94      0.95      1650\n",
      "     not acq       0.98      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.96      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9797914789548204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.96      0.95      0.95      1650\n",
      "     not acq       0.99      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.97      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9796627622602652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.96      0.95      0.95      1650\n",
      "     not acq       0.99      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.97      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9797914789548204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.95      0.95      1650\n",
      "     not acq       0.99      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.97      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy\n",
      "0.9799201956493757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.95      0.95      0.95      1650\n",
      "     not acq       0.99      0.99      0.99      6119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      7769\n",
      "   macro avg       0.97      0.97      0.97      7769\n",
      "weighted avg       0.98      0.98      0.98      7769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_to_test = [0.001,0.01,0.1,1,10,100, 1000]\n",
    "lrcs = [LogisticRegression(C=c) for c in c_to_test]\n",
    "do_multiple_10foldcrossvalidation(lrcs,trn_data,trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this case, changing the parameter from the default is not desirable. When training with fairly large datasets to solve a straightforward task with a simple classifier, the effect of regularization is often minimal.\n",
    "\n",
    "Under normal circumstances we might do more parameter tuning or feature selection (and we encourage you to play around), but let's just skip to testing the classifiers on the test set and displaying the results using matplotlib...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9304405432262338, 0.9420337860218615, 0.9549519708512753, 0.9738323948327261, 0.9817820470354423, 0.9827757535607817]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_and_graph(clfs,training_data,training_classifications,test_data,test_classifications):\n",
    "    accuracies = []\n",
    "    for clf in clfs:\n",
    "        clf.fit(training_data,training_classifications)\n",
    "        predictions = clf.predict(test_data)\n",
    "        accuracies.append(accuracy_score(test_classifications,predictions))\n",
    "    print (accuracies)\n",
    "    p = plt.bar([num + 0.25 for num in range(len(clfs))], accuracies,0.5)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy classifying acq topic in Reuters, by classifier')\n",
    "    plt.ylim([0.9,1])\n",
    "    plt.xticks([num + 0.5 for num in range(len(clfs))], ('kNN', 'DT', 'RF', 'NB', 'SVM', 'LR'))\n",
    "    plt.show()\n",
    "\n",
    "test_and_graph(clfs,trn_data,trn_classes,test_data,test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty close to what we saw using cross-validation, with Logistic Regression winning out over SVMs by a tiny margin, with an impressive accuracy of 98.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
