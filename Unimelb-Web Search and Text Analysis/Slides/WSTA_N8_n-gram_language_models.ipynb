{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# *N*-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook, we'll be building bigram *n*-gram language models from scratch. The first part of building a language model is collecting counts from corpora. We'll do some preprocessing first, by lowercasing everything and add `<s>` (start) and `</s>` (end) symbols at the beginning and end of each sentence. For bigrams, we are using dictionaries of dictionaries with the strings as keys, which is a convenient though not particularly memory efficient way to represent things. We will use the unigram counts later for doing smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def convert_sentence(sentence):\n",
    "    return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "\n",
    "def get_counts(sentences):\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    unigram_counts = Counter()\n",
    "    start_count = 0  # \"<s>\" counts: need these for bigram probs\n",
    "\n",
    "    # collect initial unigram statistics\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence(sentence)\n",
    "        for word in sentence[1:]: # from 1, so we don't generate the <s> token\n",
    "            unigram_counts[word] += 1\n",
    "        start_count += 1\n",
    "\n",
    "    # collect bigram counts\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence(sentence)\n",
    "        # generate a list of bigrams\n",
    "        bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "        # iterate over bigrams\n",
    "        for bigram in bigram_list:\n",
    "            first, second = bigram\n",
    "            bigram_counts[first][second] += 1\n",
    "            \n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "    return unigram_counts, bigram_counts, start_count, token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have counts, we can use them to generate sentences. Here we use [numpy.random.choice](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html), which allows us to randomly choose from a list based on a corresponding list of probabilities, which we calculate by normalizing the raw counts. We start with &lt;s&gt;, and generate the next word given the bigram counts which begin with &lt;s&gt;, and then use that word to generate the next word, etc. It stops when it generates an &lt;/s&gt;. We return a string with some fixes to make the sentence look a proper sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice \n",
    "\n",
    "def generate_sentence(bigram_counts):\n",
    "    current_word = \"<s>\"\n",
    "    sentence = [current_word]\n",
    "    while current_word != \"</s>\":\n",
    "        # get counts for previous word\n",
    "        prev_word = current_word\n",
    "        prev_word_counts = bigram_counts[prev_word]\n",
    "        # obtain bigram probability distribution given the previous word\n",
    "        bigram_probs = []\n",
    "        total_counts = float(sum(prev_word_counts.values()))\n",
    "        for word in prev_word_counts:\n",
    "            bigram_probs.append(prev_word_counts[word] / total_counts)\n",
    "        # sample the next word\n",
    "        current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
    "        sentence.append(current_word)\n",
    "\n",
    "    # get rid of start and end of sentence tokens\n",
    "    sentence = \" \".join(sentence[1:-1])\n",
    "    return sentence\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's try out our *n*-gram driven sentence generator on samples from two corpora: the Penn Treebank, and some out-of-copyright English literature from Project Gutenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg\n",
      "Sentence 1\n",
      "when they walk by it over the shadow ' d defeate was very uncomfortable and scatter the man , profits .\"\n",
      "Sentence 2\n",
      "but he given me the room , to the lord have been guessing how long dragging away she will not beasts that smiteth him on that lieth at last me clarified and independence , 16 : and they departed from trouble to sit on captain wentworth of the lord , and slew all people , and all the wee ' s words .\n",
      "Sentence 3\n",
      "\" what do not answer ; gershon by the sad meditation .\n",
      "Sentence 4\n",
      "you , which have enough to mount vesuvius , scotch accent : 34 and melted - spade , which staggered back in my father ,\" said , to hear , in again to jerusalem was not obey thy servant abide jehovah was over by upon thee , over it .\n",
      "Sentence 5\n",
      "banq .\n",
      "Treebank\n",
      "Sentence 1\n",
      "u.s. currency temporarily throws into a share , other cash-rich countries in its security pacific sierra , the treasury notes .\n",
      "Sentence 2\n",
      "tired of the ringers . '' says 0 the editors wrote a hundred million *u* , university school has been the region who *t*-124 pay their real estate and 0 it will want assets , '' controversy comes to date , which *t*-70 can do *t*-1 the company 's results from a bushel , 8 7\\/16 % gain from new york attorney 's construction machinery plants in a wide swings in the broader question of bell to take place .\n",
      "Sentence 3\n",
      "modifications *ich*-3 with *rnr*-1 or having the field started with any cost-benefit scrutiny have the bush will resign rather have a 50 % 150 million *u* on rally 's -rcb- the morale-damaging public companies will have jumped 21 states that after *-1 to cheat *t*-2 as well stoked *-1 to a few people have obtained even at our two have engaged *-1 with recently harvested midwest , mcgraw-hill was n't be acquired a sell-off that japan and advertisers and aerospace , the national statistics imply that hong kong , and on the orange county register were going *-3 buying u.s. wants the collar '' says 0 this year mortgage bankers acceptances from the coleman said that *t*-1 , into shares outstanding .\n",
      "Sentence 4\n",
      "but i have used * to drugs .\n",
      "Sentence 5\n",
      "now .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, treebank\n",
    "\n",
    "gutenberg_unigrams, gutenberg_bigrams, gutenberg_start_count, gutenberg_token_count = get_counts(gutenberg.sents())\n",
    "print(\"Gutenberg\")\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(generate_sentence(gutenberg_bigrams))\n",
    "    \n",
    "treebank_unigrams, treebank_bigrams, treebank_start_count, treebank_token_count = get_counts(treebank.sents())\n",
    "print(\"Treebank\")\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(generate_sentence(treebank_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generally, we can see some local coherence but most of these sentences are complete nonsense. Across the two corpora, the sentences are noticeably different, it's very obvious that the model from Project Gutenberg is trained on literature, whereas the Penn Treebank data is financial. For the latter, there are some strange tokens (those starting with \\*) we should probably have filtered out.\n",
    "\n",
    "Using language models to generate sentences is fun but not very useful. A more practical application is the ability to assign a probability to a sentence. In order to do that for anything but toy examples, however, we will need to smooth our models so it doesn't assign a zero probability to the sentence whenever it sees a bigram. Here, we'll test two fairly simple smoothing techniques, add-*k* smoothing and interpolated smoothing. In both cases, we will calculate the log probability, to avoid working with very small numbers. The functions below give the probability for a single word at index i in a sentence.\n",
    "\n",
    "Notice that interpolation is implemented using 3 probabilities: the bigram, the unigram and a \"zerogram\" probability. The \"zerogram\" actually refers to the probability of any word appearing. We need this extra probability in order to account for out-of-vocabulary (OOVs) words, which result in zero probability for both bigrams and unigrams. Estimating the probability of OOVs is a general problem: here we use an heuristic that uses a uniform distribution over all words in the vocabulary (1 / |V|)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_log_prob_addk(prev_word, word, unigram_counts, bigram_counts, k):\n",
    "    sm_bigram_counts = bigram_counts[prev_word][word] + k\n",
    "    sm_unigram_counts = unigram_counts[prev_word] + k*len(unigram_counts)\n",
    "    return math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "\n",
    "def get_log_prob_interp(prev_word, word, unigram_counts, bigram_counts, start_count, token_count, lambdas):\n",
    "    bigram_lambda = lambdas[0]\n",
    "    unigram_lambda = lambdas[1]\n",
    "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
    "    \n",
    "    # start by getting bigram probability\n",
    "    sm_bigram_counts = bigram_counts[prev_word][word] * bigram_lambda\n",
    "    if sm_bigram_counts == 0.0:\n",
    "        interp_bigram_counts = 0\n",
    "    else:\n",
    "        if prev_word == \"<s>\":\n",
    "            u_counts = start_count\n",
    "        else:\n",
    "            u_counts = unigram_counts[prev_word]\n",
    "        interp_bigram_counts = sm_bigram_counts / float(u_counts)\n",
    "        \n",
    "    # unigram probability\n",
    "    interp_unigram_counts = (unigram_counts[word] / token_count) * unigram_lambda\n",
    "    \n",
    "    # \"zerogram\" probability: this is to account for out-of-vocabulary words\n",
    "    # this is just 1 / |V|\n",
    "    vocab_size = len(unigram_counts)\n",
    "    interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
    "    \n",
    "    return math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Extending this to calculate the probability of an entire sentence is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.460406447015146\n",
      "-49.538820071127226\n",
      "-39.776378681452364\n",
      "-39.245480234555636\n"
     ]
    }
   ],
   "source": [
    "def get_sent_log_prob_addk(sentence, unigram_counts, bigram_counts, start_count, token_count, k):\n",
    "    sentence = convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    return sum([get_log_prob_addk(prev_word, \n",
    "                                  word, \n",
    "                                  unigram_counts, \n",
    "                                  bigram_counts, \n",
    "                                  k) for prev_word, word in bigram_list])\n",
    "\n",
    "def get_sent_log_prob_interp(sentence, unigram_counts, bigram_counts, start_count, token_count, lambdas):\n",
    "    sentence = convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    return sum([get_log_prob_interp(prev_word, \n",
    "                                    word, \n",
    "                                    unigram_counts, \n",
    "                                    bigram_counts,\n",
    "                                    start_count,\n",
    "                                    token_count, \n",
    "                                    lambdas) for prev_word, word in bigram_list])\n",
    "    \n",
    "sentence = \"revenue increased last quarter .\".split()\n",
    "print(get_sent_log_prob_addk(sentence, gutenberg_unigrams, gutenberg_bigrams, gutenberg_start_count,\n",
    "                             gutenberg_token_count, 0.05))\n",
    "print(get_sent_log_prob_interp(sentence, \n",
    "                               gutenberg_unigrams, \n",
    "                               gutenberg_bigrams, \n",
    "                               gutenberg_start_count, \n",
    "                               gutenberg_token_count, \n",
    "                               (0.8, 0.19)))\n",
    "print(get_sent_log_prob_addk(sentence, treebank_unigrams, treebank_bigrams, treebank_start_count,\n",
    "                             treebank_token_count, 0.05))\n",
    "print(get_sent_log_prob_interp(sentence, \n",
    "                               treebank_unigrams, \n",
    "                               treebank_bigrams, \n",
    "                               treebank_start_count, \n",
    "                               treebank_token_count, \n",
    "                               (0.8, 0.19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output for our sample sentence looks reasonable, in particular using the Treebank model results in a noticeably higher probability, which is what we'd expect given the input sentence. The differences in probability between the different smoothing techniques is more modest (though keep in mind this is a logrithmic scale). Now, let's use perplexity to evaluate different smoothing techniques at the level of the corpus. For this, we'll use the Brown corpus again, dividing it up randomly into a training set and a test set based on an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from random import shuffle\n",
    "\n",
    "sents = list(brown.sents())\n",
    "shuffle(sents)\n",
    "cutoff = int(0.8*len(sents))\n",
    "training_set = sents[:cutoff]\n",
    "test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "\n",
    "brown_unigrams, brown_bigrams, brown_start_count, brown_token_count = get_counts(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since our probabilities are in log space, we will calculate perplexity in log space as well, then take the exponential at the end\n",
    "\n",
    "$PP(W) = \\sqrt[m]{\\frac{1}{P(W)}}$\n",
    "\n",
    "$\\log{PP(W)} = -\\frac{1}{m} \\log{P(W)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentences, unigram_counts, bigram_counts, start_count, \n",
    "                         token_count, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence,\n",
    "                                             unigram_counts,\n",
    "                                             bigram_counts,\n",
    "                                             start_count,\n",
    "                                             token_count,\n",
    "                                             parameter)\n",
    "    return math.exp(-total_log_prob / test_token_count)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see how our two smoothing techniques do with a range of possible parameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add k\n",
      "0.0001\n",
      "731.3161527556367\n",
      "0.001\n",
      "595.8735785891397\n",
      "0.01\n",
      "702.6888698937582\n",
      "0.05\n",
      "1017.4581429532024\n",
      "0.2\n",
      "1661.9663636138337\n",
      "1.0\n",
      "3460.5399802700067\n",
      "interpolation\n",
      "(0.98, 0.010000000000000009)\n",
      "749.3689266396871\n",
      "(0.95, 0.040000000000000036)\n",
      "575.9950692703219\n",
      "(0.75, 0.24)\n",
      "420.0872108058523\n",
      "(0.5, 0.49)\n",
      "418.2059193589532\n",
      "(0.25, 0.74)\n",
      "492.954545454064\n",
      "(0.001, 0.989)\n",
      "971.6580723471634\n"
     ]
    }
   ],
   "source": [
    "print(\"add k\")\n",
    "for k in [0.0001,0.001,0.01, 0.05,0.2,1.0]:\n",
    "    print(k)\n",
    "    print(calculate_perplexity(test_set,\n",
    "                               brown_unigrams,\n",
    "                               brown_bigrams,\n",
    "                               brown_start_count,\n",
    "                               brown_token_count,\n",
    "                               get_sent_log_prob_addk,\n",
    "                               k))\n",
    "print(\"interpolation\")\n",
    "for bigram_lambda in [0.98,0.95,0.75,0.5,0.25,0.001]:\n",
    "    unigram_lambda = 0.99 - bigram_lambda\n",
    "    lambdas = (bigram_lambda, unigram_lambda)\n",
    "    print(lambdas) \n",
    "    print(calculate_perplexity(test_set, \n",
    "                               brown_unigrams, \n",
    "                               brown_bigrams,\n",
    "                               brown_start_count,\n",
    "                               brown_token_count, \n",
    "                               get_sent_log_prob_interp, \n",
    "                               lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our results indicate that, with regards to perplexity, interpolation is generally better than add k. Very low (though not too low) k is preferred for add k, wheres our best lambdas is in the middle of the range, though apparently with a small preference for more weight on the bigram probability, which makes sense.\n",
    "\n",
    "From the basis given here, you can try playing around with some of the other corpora in NLTK and see if you get similar results. You could also implement a trigram model, or another kind of smoothing, to see if you can get better perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
